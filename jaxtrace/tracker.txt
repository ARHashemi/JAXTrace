"""
Particle Tracker Module for JAXTrace

Memory-optimized particle tracking with JAX acceleration and multiple integration methods.
"""

import numpy as np
from typing import List, Dict, Optional, Tuple, Callable, Union
import gc
import psutil
import functools
from .reader import VTKReader

# JAX imports with error handling
try:
    import jax
    import jax.numpy as jnp
    from jax import jit, vmap, lax
    JAX_AVAILABLE = True
except ImportError:
    JAX_AVAILABLE = False
    print("Warning: JAX not available. ParticleTracker will use CPU fallback.")


class ParticleTracker:
    """Memory-optimized particle tracker with JAX acceleration."""
    
    def __init__(self, 
                 vtk_reader: VTKReader,
                 max_gpu_memory_gb: float = 6.0,
                 k_neighbors: int = 8,
                 shape_function: str = 'linear',
                 interpolation_method: str = 'finite_element'):
        """
        Initialize particle tracker.
        
        Args:
            vtk_reader: VTKReader instance with loaded grid data
            max_gpu_memory_gb: Maximum GPU memory to use in GB
            k_neighbors: Number of neighbors for finite element interpolation
            shape_function: Shape function type ('linear', 'quadratic', 'cubic', 'gaussian')
            interpolation_method: 'finite_element' or 'nearest_neighbor'
        """
        if not JAX_AVAILABLE:
            raise ImportError("JAX is required for ParticleTracker. Please install JAX.")
        
        self.vtk_reader = vtk_reader
        self.max_gpu_memory_gb = max_gpu_memory_gb
        self.k_neighbors = k_neighbors
        self.shape_function = shape_function
        self.interpolation_method = interpolation_method
        
        # Get grid information
        grid_info = vtk_reader.get_grid_info()
        self.grid_points = jnp.array(grid_info['grid_points'])
        self.n_timesteps = grid_info['n_timesteps']
        self.grid_bounds = grid_info['bounds']
        
        # Compile core functions
        self._compile_interpolation_functions()
        self._compile_integration_functions()
        
        # Set interpolation method
        self._set_interpolation_method()
        
        # Memory tracking
        self.current_gpu_memory = 0
        
        print(f"ParticleTracker initialized with {self.interpolation_method} interpolation")
        print(f"Grid: {len(self.grid_points)} points, {self.n_timesteps} timesteps")
    
    def _compile_interpolation_functions(self):
        """Compile interpolation functions for velocity field sampling."""
        
        @jit
        def nearest_neighbor_velocity(position: jnp.ndarray, velocity_field: jnp.ndarray) -> jnp.ndarray:
            """Fast nearest neighbor velocity interpolation."""
            distances = jnp.linalg.norm(self.grid_points - position, axis=1)
            nearest_idx = jnp.argmin(distances)
            return velocity_field[nearest_idx]
        
        def create_finite_element_velocity(k_neighbors: int, shape_function: str):
            """Create a finite element velocity function with fixed parameters."""
            
            @jit
            def finite_element_velocity(position: jnp.ndarray, velocity_field: jnp.ndarray) -> jnp.ndarray:
                """Finite element velocity interpolation using shape functions."""
                distances = jnp.linalg.norm(self.grid_points - position, axis=1)
                
                # Find k nearest neighbors
                neighbor_indices = jnp.argpartition(distances, k_neighbors)[:k_neighbors]
                neighbor_distances = distances[neighbor_indices]
                neighbor_points = self.grid_points[neighbor_indices]
                neighbor_velocities = velocity_field[neighbor_indices]
                
                # Avoid division by zero
                safe_distances = jnp.maximum(neighbor_distances, 1e-10)
                
                # Calculate weights based on shape function (resolved at compile time)
                if shape_function == 'linear':
                    weights = 1.0 / safe_distances
                elif shape_function == 'quadratic':
                    weights = 1.0 / (safe_distances**2)
                elif shape_function == 'cubic':
                    weights = 1.0 / (safe_distances**3)
                elif shape_function == 'gaussian':
                    sigma = jnp.mean(safe_distances)
                    weights = jnp.exp(-0.5 * (safe_distances / sigma)**2)
                else:
                    # Default to linear
                    weights = 1.0 / safe_distances
                
                weights = weights / jnp.sum(weights)
                
                # Interpolate velocity
                interpolated_velocity = jnp.sum(weights[:, None] * neighbor_velocities, axis=0)
                return interpolated_velocity
            
            return finite_element_velocity
        
        @jit
        def temporal_velocity_interpolation(position: jnp.ndarray, 
                                          velocity_field_t1: jnp.ndarray,
                                          velocity_field_t2: jnp.ndarray,
                                          alpha: float) -> jnp.ndarray:
            """Temporally interpolate velocity between two time steps."""
            vel_t1 = self.interpolate_velocity(position, velocity_field_t1)
            vel_t2 = self.interpolate_velocity(position, velocity_field_t2)
            return (1 - alpha) * vel_t1 + alpha * vel_t2
        
        # Store compiled functions
        self.nearest_neighbor_velocity = nearest_neighbor_velocity
        self.finite_element_velocity = create_finite_element_velocity(self.k_neighbors, self.shape_function)
        self.temporal_velocity_interpolation = temporal_velocity_interpolation
    
    def _compile_integration_functions(self):
        """Compile integration methods for particle advection."""
        
        @jit
        def get_velocity_at_time(position: jnp.ndarray, 
                               time_fraction: float,
                               velocity_field_current: jnp.ndarray,
                               velocity_field_next: jnp.ndarray) -> jnp.ndarray:
            """Get velocity at intermediate time steps for RK methods."""
            # Use JAX conditional operations for traced arrays
            # Package all arguments for the conditional functions
            args = (position, velocity_field_current, velocity_field_next, time_fraction)
            
            return lax.cond(
                time_fraction <= 0.0,
                lambda args: self.interpolate_velocity(args[0], args[1]),  # position, velocity_field_current
                lambda args: lax.cond(
                    args[3] >= 1.0,  # time_fraction >= 1.0
                    lambda args: self.interpolate_velocity(args[0], args[2]),  # position, velocity_field_next
                    lambda args: self.temporal_velocity_interpolation(args[0], args[1], args[2], args[3]),
                    args
                ),
                args
            )
        
        @jit
        def euler_step(position: jnp.ndarray, velocity: jnp.ndarray, dt: float) -> jnp.ndarray:
            """Simple Euler integration step."""
            return position + dt * velocity
        
        @jit
        def rk2_step(position: jnp.ndarray, 
                    velocity_field_current: jnp.ndarray,
                    velocity_field_next: jnp.ndarray,
                    dt: float) -> jnp.ndarray:
            """2nd order Runge-Kutta step with temporal interpolation."""
            k1 = get_velocity_at_time(position, 0.0, velocity_field_current, velocity_field_next)
            k2_pos = position + 0.5 * dt * k1
            k2 = get_velocity_at_time(k2_pos, 0.5, velocity_field_current, velocity_field_next)
            return position + dt * k2
        
        @jit
        def rk4_step(position: jnp.ndarray, 
                    velocity_field_current: jnp.ndarray,
                    velocity_field_next: jnp.ndarray,
                    dt: float) -> jnp.ndarray:
            """4th order Runge-Kutta step with proper temporal interpolation."""
            k1 = get_velocity_at_time(position, 0.0, velocity_field_current, velocity_field_next)
            
            k2_pos = position + 0.5 * dt * k1
            k2 = get_velocity_at_time(k2_pos, 0.5, velocity_field_current, velocity_field_next)
            
            k3_pos = position + 0.5 * dt * k2
            k3 = get_velocity_at_time(k3_pos, 0.5, velocity_field_current, velocity_field_next)
            
            k4_pos = position + dt * k3
            k4 = get_velocity_at_time(k4_pos, 1.0, velocity_field_current, velocity_field_next)
            
            return position + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)
        
        @jit
        def rk4_step_static(position: jnp.ndarray, velocity_field: jnp.ndarray, dt: float) -> jnp.ndarray:
            """4th order Runge-Kutta step for static velocity fields."""
            k1 = self.interpolate_velocity(position, velocity_field)
            k2 = self.interpolate_velocity(position + 0.5 * dt * k1, velocity_field)
            k3 = self.interpolate_velocity(position + 0.5 * dt * k2, velocity_field)
            k4 = self.interpolate_velocity(position + dt * k3, velocity_field)
            return position + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)
        
        # Store compiled functions
        self.get_velocity_at_time = get_velocity_at_time
        self.euler_step = euler_step
        self.rk2_step = rk2_step
        self.rk4_step = rk4_step
        self.rk4_step_static = rk4_step_static
        
        # Create vectorized versions
        self.batch_euler = vmap(euler_step, in_axes=(0, 0, None))
        self.batch_rk2 = vmap(self.rk2_step, in_axes=(0, None, None, None))
        self.batch_rk4 = vmap(self.rk4_step, in_axes=(0, None, None, None))
        self.batch_rk4_static = vmap(self.rk4_step_static, in_axes=(0, None, None))
    
    def _set_interpolation_method(self):
        """Set the interpolation method function."""
        if self.interpolation_method == 'nearest_neighbor':
            self.interpolate_velocity = self.nearest_neighbor_velocity
        elif self.interpolation_method == 'finite_element':
            # Use the pre-compiled finite element function
            self.interpolate_velocity = self.finite_element_velocity
        else:
            raise ValueError(f"Unknown interpolation method: {self.interpolation_method}")
    
    def _estimate_batch_size(self, n_particles: int, integration_method: str = 'euler') -> int:
        """Estimate optimal batch size based on available GPU memory."""
        bytes_per_float = 4  # float32
        
        # Base memory for particles (position + velocity)
        base_memory_per_particle = 6 * bytes_per_float  # 3D position + 3D velocity
        
        # Additional memory for integration method
        if integration_method in ['rk2', 'rk4']:
            base_memory_per_particle *= 4  # Much higher for RK methods due to intermediate steps
        
        # Grid points memory (already loaded)
        grid_memory = self.grid_points.nbytes
        
        # Velocity field memory (current + next for RK methods)
        velocity_field_memory = grid_memory  # Assume same size as grid
        if integration_method in ['rk2', 'rk4']:
            velocity_field_memory *= 2  # Current + next velocity fields
        
        # Available memory for batch processing (be more conservative)
        available_memory = self.max_gpu_memory_gb * 0.4 * 1024**3  # Only 40% of max, in bytes
        available_memory -= grid_memory + velocity_field_memory
        
        # Ensure we have some minimum memory available
        if available_memory <= 0:
            print(f"Warning: Very low available memory. Using minimum batch size.")
            return min(100, n_particles)
        
        # Calculate batch size
        max_batch_size = int(available_memory / base_memory_per_particle)
        
        # Conservative limits based on integration method
        if integration_method == 'euler':
            max_reasonable = 20000
        elif integration_method == 'rk2':
            max_reasonable = 10000
        else:  # rk4
            max_reasonable = 5000
        
        # Apply limits
        batch_size = min(max_batch_size, n_particles, max_reasonable)
        batch_size = max(batch_size, 50)  # Minimum 50 particles
        
        print(f"Estimated batch size for {integration_method}: {batch_size} particles")
        return batch_size  # Fix: Added missing return statement
    
    def check_memory_requirements(self, n_particles: int, integration_method: str = 'euler') -> Dict:
        """
        Check if the given configuration will fit in memory and provide recommendations.
        
        Args:
            n_particles: Number of particles to track
            integration_method: Integration method to use
            
        Returns:
            Dictionary with memory analysis and recommendations
        """
        bytes_per_float = 4
        
        # Estimate memory requirements
        particle_memory = n_particles * 6 * bytes_per_float  # position + velocity
        if integration_method in ['rk2', 'rk4']:
            particle_memory *= 4  # Intermediate steps
        
        grid_memory = self.grid_points.nbytes
        velocity_memory = grid_memory * (2 if integration_method in ['rk2', 'rk4'] else 1)
        
        total_memory_gb = (particle_memory + grid_memory + velocity_memory) / 1024**3
        available_memory_gb = self.max_gpu_memory_gb * 0.4  # Conservative estimate
        
        analysis = {
            'n_particles': n_particles,
            'integration_method': integration_method,
            'estimated_memory_gb': total_memory_gb,
            'available_memory_gb': available_memory_gb,
            'will_fit': total_memory_gb <= available_memory_gb,
            'recommendations': []
        }
        
        if not analysis['will_fit']:
            memory_ratio = total_memory_gb / available_memory_gb
            
            # Particle count reduction
            recommended_particles = int(n_particles / memory_ratio * 0.8)  # 20% safety margin
            analysis['recommendations'].append(f"Reduce particles to ~{recommended_particles}")
            
            # Integration method downgrade
            if integration_method == 'rk4':
                analysis['recommendations'].append("Consider using 'rk2' integration (4x less memory)")
            elif integration_method == 'rk2':
                analysis['recommendations'].append("Consider using 'euler' integration (4x less memory)")
            
            # Subsampling
            subsample_factor = max(2, int(np.ceil(memory_ratio)))
            analysis['recommendations'].append(f"Use spatial subsampling factor of {subsample_factor}")
            
            # Batch processing
            analysis['recommendations'].append("Process in smaller time chunks")
        
        return analysis
    
    def auto_optimize_for_memory(self, 
                                initial_positions: jnp.ndarray,
                                integration_method: str = 'euler') -> Tuple[jnp.ndarray, str, Dict]:
        """
        Automatically optimize particle tracking configuration for available memory.
        
        Args:
            initial_positions: Initial particle positions
            integration_method: Preferred integration method
            
        Returns:
            Tuple of (optimized_positions, optimized_method, optimization_info)
        """
        n_particles = len(initial_positions)
        
        # Check if current configuration fits
        analysis = self.check_memory_requirements(n_particles, integration_method)
        
        if analysis['will_fit']:
            return initial_positions, integration_method, {'optimized': False, 'original_fit': True}
        
        print(f"Configuration requires {analysis['estimated_memory_gb']:.2f} GB but only {analysis['available_memory_gb']:.2f} GB available")
        print("Auto-optimizing...")
        
        optimization_info = {'optimized': True, 'original_fit': False, 'changes': []}
        optimized_positions = initial_positions
        optimized_method = integration_method
        
        # Try reducing integration method first
        if integration_method == 'rk4':
            test_analysis = self.check_memory_requirements(n_particles, 'rk2')
            if test_analysis['will_fit']:
                optimized_method = 'rk2'
                optimization_info['changes'].append(f"Integration: {integration_method} → rk2")
            else:
                test_analysis = self.check_memory_requirements(n_particles, 'euler')
                if test_analysis['will_fit']:
                    optimized_method = 'euler'
                    optimization_info['changes'].append(f"Integration: {integration_method} → euler")
        elif integration_method == 'rk2':
            test_analysis = self.check_memory_requirements(n_particles, 'euler')
            if test_analysis['will_fit']:
                optimized_method = 'euler'
                optimization_info['changes'].append(f"Integration: {integration_method} → euler")
        
        # If still doesn't fit, reduce particle count
        final_analysis = self.check_memory_requirements(len(optimized_positions), optimized_method)
        if not final_analysis['will_fit']:
            memory_ratio = final_analysis['estimated_memory_gb'] / final_analysis['available_memory_gb']
            reduction_factor = int(np.ceil(memory_ratio * 1.2))  # 20% safety margin
            
            # Subsample particles
            subsample_indices = jnp.arange(0, len(optimized_positions), reduction_factor)
            optimized_positions = optimized_positions[subsample_indices]
            optimization_info['changes'].append(f"Particles: {n_particles} → {len(optimized_positions)} (factor {reduction_factor})")
        
        print(f"Optimizations applied: {'; '.join(optimization_info['changes'])}")
        return optimized_positions, optimized_method, optimization_info
    
    def create_particle_grid(self, 
                           resolution: Tuple[int, int, int], 
                           bounds_padding: float = 0.1,
                           box_bounds: Optional[Tuple[Tuple[float, float], ...]] = None) -> jnp.ndarray:
        """
        Create initial particle grid within specified bounds.
        
        Args:
            resolution: Grid resolution (nx, ny, nz)
            bounds_padding: Padding factor for automatic bounds (ignored if box_bounds provided)
            box_bounds: Custom bounds as ((xmin, xmax), (ymin, ymax), (zmin, zmax))
        
        Returns:
            Initial particle positions array (n_particles, 3)
        """
        if box_bounds is not None:
            # Use custom bounds
            (xmin, xmax), (ymin, ymax), (zmin, zmax) = box_bounds
            print(f"Using custom box bounds: x=[{xmin:.2f}, {xmax:.2f}], "
                  f"y=[{ymin:.2f}, {ymax:.2f}], z=[{zmin:.2f}, {zmax:.2f}]")
        else:
            # Use grid bounds with padding
            if self.grid_bounds is None:
                raise ValueError("Grid bounds not available and no custom bounds provided")
            
            grid_xmin, grid_xmax, grid_ymin, grid_ymax, grid_zmin, grid_zmax = self.grid_bounds
            
            # Add padding
            x_pad = (grid_xmax - grid_xmin) * bounds_padding
            y_pad = (grid_ymax - grid_ymin) * bounds_padding
            z_pad = (grid_zmax - grid_zmin) * bounds_padding
            
            xmin = grid_xmin + x_pad
            xmax = grid_xmax - x_pad
            ymin = grid_ymin + y_pad
            ymax = grid_ymax - y_pad
            zmin = grid_zmin + z_pad
            zmax = grid_zmax - z_pad
            
            print(f"Using grid bounds with padding: x=[{xmin:.2f}, {xmax:.2f}], "
                  f"y=[{ymin:.2f}, {ymax:.2f}], z=[{zmin:.2f}, {zmax:.2f}]")
        
        nx, ny, nz = resolution
        x = jnp.linspace(xmin, xmax, nx)
        y = jnp.linspace(ymin, ymax, ny)
        z = jnp.linspace(zmin, zmax, nz)
        
        X, Y, Z = jnp.meshgrid(x, y, z, indexing='ij')
        positions = jnp.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1)
        
        print(f"Created {len(positions)} particles in {nx}×{ny}×{nz} grid")
        
        return positions
    
    def track_particles(self, 
                       initial_positions: jnp.ndarray, 
                       dt: float, 
                       n_steps: int,
                       integration_method: str = 'euler',
                       time_step_stride: int = 1,
                       save_trajectories: bool = False,
                       progress_callback: Optional[Callable] = None,
                       output_frequency: int = 100) -> Union[jnp.ndarray, Tuple[jnp.ndarray, jnp.ndarray]]:
        """
        Track particles through time with streaming memory optimization.
        
        Args:
            initial_positions: Initial particle positions (N, 3)
            dt: Time step size
            n_steps: Number of integration steps
            integration_method: 'euler', 'rk2', or 'rk4'
            time_step_stride: Stride through time steps (>1 for temporal subsampling)
            save_trajectories: Whether to save full particle trajectories
            progress_callback: Function called with (step, positions)
            output_frequency: How often to call progress callback
            
        Returns:
            If save_trajectories=False: Final particle positions (N, 3)
            If save_trajectories=True: (final_positions, trajectories) where trajectories is (N, n_steps+1, 3)
        """
        # Input validation
        if initial_positions is None or len(initial_positions) == 0:
            raise ValueError("initial_positions cannot be None or empty")
        
        n_particles = initial_positions.shape[0]
        
        # Validate integration method
        valid_methods = ['euler', 'rk2', 'rk4']
        if integration_method not in valid_methods:
            raise ValueError(f"integration_method must be one of {valid_methods}, got {integration_method}")
        
        # Estimate batch size with error handling
        try:
            batch_size = self._estimate_batch_size(n_particles, integration_method)
        except Exception as e:
            print(f"Error estimating batch size: {e}")
            print("Using fallback batch size calculation...")
            # Fallback batch size calculation
            if integration_method == 'euler':
                batch_size = min(1000, n_particles)
            elif integration_method == 'rk2':
                batch_size = min(500, n_particles)
            else:  # rk4
                batch_size = min(250, n_particles)
        
        # Final safety check
        if batch_size is None or batch_size <= 0:
            print("Warning: Invalid batch size. Using conservative fallback.")
            batch_size = min(100, n_particles)
        
        # Ensure batch_size is an integer
        batch_size = int(batch_size)
        
        # Additional memory checks
        if n_particles > 100000 and integration_method in ['rk2', 'rk4']:
            print(f"Warning: Large particle count ({n_particles}) with {integration_method} may cause memory issues.")
            print("Consider using 'euler' integration or reducing particle count.")
        
        print(f"Tracking {n_particles} particles for {n_steps} steps")
        print(f"Integration method: {integration_method}, batch size: {batch_size}")
        print(f"Time step stride: {time_step_stride}")
        
        # Select integration function
        if integration_method == 'euler':
            integrate_batch = self.batch_euler
        elif integration_method == 'rk2':
            integrate_batch = self.batch_rk2
        elif integration_method == 'rk4':
            integrate_batch = self.batch_rk4
        else:
            raise ValueError(f"Unknown integration method: {integration_method}")
        
        current_positions = initial_positions
        current_time_idx = 0
        
        # Initialize trajectory storage if requested
        if save_trajectories:
            trajectories = jnp.zeros((n_particles, n_steps + 1, 3))
            trajectories = trajectories.at[:, 0].set(initial_positions)
        
        # Pre-allocate velocity fields to avoid repeated loading
        velocity_field = None
        velocity_field_next = None
        
        for step in range(n_steps):
            try:
                # Load velocity fields with memory management
                if velocity_field is None or step % 5 == 0:  # Reload every 5 steps to manage cache
                    velocity_field = jnp.array(self.vtk_reader.load_single_timestep(current_time_idx))
                
                # Get next velocity field for temporal interpolation (if available)
                next_time_idx = min(current_time_idx + time_step_stride, self.n_timesteps - 1)
                if next_time_idx != current_time_idx and integration_method in ['rk2', 'rk4']:
                    if velocity_field_next is None or step % 5 == 0:
                        velocity_field_next = jnp.array(self.vtk_reader.load_single_timestep(next_time_idx))
                else:
                    velocity_field_next = velocity_field
                
                # Process particles in batches with more conservative batching
                new_positions = []
                n_batches = (n_particles + batch_size - 1) // batch_size
                
                for batch_idx in range(n_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min((batch_idx + 1) * batch_size, n_particles)
                    batch_positions = current_positions[start_idx:end_idx]
                    
                    try:
                        if integration_method == 'euler':
                            batch_velocities = vmap(self.interpolate_velocity, 
                                                  in_axes=(0, None))(batch_positions, velocity_field)
                            new_batch = integrate_batch(batch_positions, batch_velocities, dt)
                        else:  # rk2 or rk4
                            new_batch = integrate_batch(batch_positions, velocity_field, velocity_field_next, dt)
                        
                        new_positions.append(new_batch)
                        
                    except Exception as e:
                        if "RESOURCE_EXHAUSTED" in str(e) or "Out of memory" in str(e):
                            print(f"Memory error at batch {batch_idx}/{n_batches}. Reducing batch size.")
                            # Reduce batch size and retry
                            smaller_batch_size = max(batch_size // 2, 10)
                            print(f"Retrying with smaller batch size: {smaller_batch_size}")
                            
                            # Process this batch with smaller sub-batches
                            batch_new_positions = []
                            sub_batches = (len(batch_positions) + smaller_batch_size - 1) // smaller_batch_size
                            
                            for sub_batch_idx in range(sub_batches):
                                sub_start = sub_batch_idx * smaller_batch_size
                                sub_end = min((sub_batch_idx + 1) * smaller_batch_size, len(batch_positions))
                                sub_batch_positions = batch_positions[sub_start:sub_end]
                                
                                if integration_method == 'euler':
                                    sub_batch_velocities = vmap(self.interpolate_velocity, 
                                                              in_axes=(0, None))(sub_batch_positions, velocity_field)
                                    sub_new_batch = integrate_batch(sub_batch_positions, sub_batch_velocities, dt)
                                else:
                                    sub_new_batch = integrate_batch(sub_batch_positions, velocity_field, velocity_field_next, dt)
                                
                                batch_new_positions.append(sub_new_batch)
                            
                            new_batch = jnp.concatenate(batch_new_positions, axis=0)
                            new_positions.append(new_batch)
                        else:
                            raise e
                
                current_positions = jnp.concatenate(new_positions, axis=0)
                
                # Save trajectory if requested
                if save_trajectories:
                    trajectories = trajectories.at[:, step + 1].set(current_positions)
                
                # Update time index
                current_time_idx = next_time_idx
                
                # Progress reporting
                if (step + 1) % output_frequency == 0:
                    print(f"Step {step + 1}/{n_steps} (time_idx: {current_time_idx})")
                    if progress_callback:
                        progress_callback(step + 1, current_positions)
                
                # Cleanup to free memory more aggressively
                if step % 10 == 0:
                    gc.collect()
                    # Clear velocity field cache periodically
                    if hasattr(self.vtk_reader, 'velocity_cache') and len(self.vtk_reader.velocity_cache) > 3:
                        self.vtk_reader.clear_cache()
                
            except Exception as e:
                if "RESOURCE_EXHAUSTED" in str(e) or "Out of memory" in str(e):
                    print(f"Critical memory error at step {step}. Suggestions:")
                    print("1. Reduce particle count or use subsampling")
                    print("2. Use 'euler' instead of 'rk2'/'rk4' integration")
                    print("3. Increase max_gpu_memory_gb setting")
                    print("4. Process in smaller chunks")
                    raise MemoryError(f"GPU memory exhausted at step {step}. See suggestions above.") from e
                else:
                    raise e
        
        if save_trajectories:
            return current_positions, trajectories
        else:
            return current_positions
    
    def track_particles_with_subsampling(self, 
                                       initial_positions: jnp.ndarray,
                                       dt: float, 
                                       n_steps: int,
                                       spatial_subsample_factor: int = 2,
                                       temporal_subsample_factor: int = 2,
                                       integration_method: str = 'euler',
                                       **kwargs) -> jnp.ndarray:
        """
        Track particles with spatial and temporal subsampling for memory efficiency.
        
        Args:
            initial_positions: Initial particle positions
            dt: Time step size
            n_steps: Number of steps
            spatial_subsample_factor: Factor for spatial subsampling of particles
            temporal_subsample_factor: Factor for temporal subsampling of time steps
            integration_method: Integration method to use
            **kwargs: Additional arguments passed to track_particles
            
        Returns:
            Final positions of subsampled particles
        """
        # Spatial subsampling
        n_particles = initial_positions.shape[0]
        subsample_indices = jnp.arange(0, n_particles, spatial_subsample_factor)
        subsampled_positions = initial_positions[subsample_indices]
        
        print(f"Spatial subsampling: {n_particles} -> {len(subsampled_positions)} particles")
        
        # Temporal subsampling
        effective_dt = dt * temporal_subsample_factor
        effective_n_steps = n_steps // temporal_subsample_factor
        
        print(f"Temporal subsampling: dt={dt}->{effective_dt}, steps={n_steps}->{effective_n_steps}")
        
        return self.track_particles(
            subsampled_positions, 
            effective_dt, 
            effective_n_steps,
            integration_method=integration_method,
            time_step_stride=temporal_subsample_factor,
            **kwargs
        )
    
    def analyze_interpolation_accuracy(self, 
                                     test_positions: jnp.ndarray,
                                     time_idx: int = 0,
                                     methods_to_test: List[str] = ['nearest_neighbor', 'finite_element']) -> Dict:
        """
        Analyze and compare interpolation accuracy for different methods.
        
        Args:
            test_positions: Test positions for interpolation (N, 3)
            time_idx: Time step index to use for testing
            methods_to_test: List of interpolation methods to compare
            
        Returns:
            Dictionary with accuracy metrics for each method
        """
        velocity_field = jnp.array(self.vtk_reader.load_single_timestep(time_idx))
        results = {}
        
        for method in methods_to_test:
            print(f"Testing {method} interpolation...")
            
            if method == 'nearest_neighbor':
                interpolated_vels = vmap(self.nearest_neighbor_velocity, 
                                       in_axes=(0, None))(test_positions, velocity_field)
            elif method == 'finite_element':
                # Create a temporary finite element function for testing
                test_fe_func = self._create_finite_element_function_for_testing()
                interpolated_vels = vmap(test_fe_func, in_axes=(0, None))(test_positions, velocity_field)
            else:
                continue
            
            # Calculate accuracy metrics
            velocity_magnitudes = jnp.linalg.norm(interpolated_vels, axis=1)
            
            results[method] = {
                'mean_velocity_magnitude': float(jnp.mean(velocity_magnitudes)),
                'std_velocity_magnitude': float(jnp.std(velocity_magnitudes)),
                'max_velocity_magnitude': float(jnp.max(velocity_magnitudes)),
                'min_velocity_magnitude': float(jnp.min(velocity_magnitudes)),
                'mean_velocity_components': [float(x) for x in jnp.mean(interpolated_vels, axis=0)],
                'std_velocity_components': [float(x) for x in jnp.std(interpolated_vels, axis=0)]
            }
        
        return results
    
    def _create_finite_element_function_for_testing(self):
        """Create a finite element function for testing purposes."""
        k_neighbors = self.k_neighbors
        shape_function = self.shape_function
        
        @jit
        def test_finite_element_velocity(position: jnp.ndarray, velocity_field: jnp.ndarray) -> jnp.ndarray:
            """Finite element velocity interpolation for testing."""
            distances = jnp.linalg.norm(self.grid_points - position, axis=1)
            
            # Find k nearest neighbors
            neighbor_indices = jnp.argpartition(distances, k_neighbors)[:k_neighbors]
            neighbor_distances = distances[neighbor_indices]
            neighbor_velocities = velocity_field[neighbor_indices]
            
            # Avoid division by zero
            safe_distances = jnp.maximum(neighbor_distances, 1e-10)
            
            # Calculate weights based on shape function (resolved at compile time)
            if shape_function == 'linear':
                weights = 1.0 / safe_distances
            elif shape_function == 'quadratic':
                weights = 1.0 / (safe_distances**2)
            elif shape_function == 'cubic':
                weights = 1.0 / (safe_distances**3)
            elif shape_function == 'gaussian':
                sigma = jnp.mean(safe_distances)
                weights = jnp.exp(-0.5 * (safe_distances / sigma)**2)
            else:
                # Default to linear
                weights = 1.0 / safe_distances
            
            weights = weights / jnp.sum(weights)
            
            # Interpolate velocity
            interpolated_velocity = jnp.sum(weights[:, None] * neighbor_velocities, axis=0)
            return interpolated_velocity
        
        return test_finite_element_velocity
    
    def get_tracker_info(self) -> Dict:
        """
        Get comprehensive tracker information.
        
        Returns:
            Dictionary containing tracker configuration and status
        """
        return {
            'interpolation_method': self.interpolation_method,
            'shape_function': self.shape_function,
            'k_neighbors': self.k_neighbors,
            'max_gpu_memory_gb': self.max_gpu_memory_gb,
            'grid_points': len(self.grid_points),
            'n_timesteps': self.n_timesteps,
            'grid_bounds': self.grid_bounds,
            'jax_available': JAX_AVAILABLE,
            'vtk_reader_cache_size': len(self.vtk_reader.velocity_cache)
        }
    
    def __repr__(self) -> str:
        """String representation of the ParticleTracker."""
        return (f"ParticleTracker(interpolation='{self.interpolation_method}', "
                f"shape_function='{self.shape_function}', "
                f"k_neighbors={self.k_neighbors}, "
                f"grid_points={len(self.grid_points)})")